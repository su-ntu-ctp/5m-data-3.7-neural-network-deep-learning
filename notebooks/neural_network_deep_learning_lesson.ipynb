{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82bb2e4a-219c-4da8-bad1-bc7c62514050",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "A neural network is a system of interconnected neurons (nodes) that work together to process complex data. These networks can learn and make intelligent decisions on their own. The basic idea behind neural networks is inspired by the workings of the human brain, though the analogy is not precise.\n",
    "\n",
    "Neural networks consist of layers of nodes: an input layer, one or more hidden layers, and an output layer. Each node in a hidden layer transforms the values from the previous layer with a weighted linear summation followed by a non-linear activation function.\n",
    "\n",
    "The 1980s saw a resurgence in neural network research, with the introduction of the backpropagation algorithm, which efficiently computes gradients for the network's weights. This allowed neural networks to learn from data and adjust their weights to improve predictions, making them useful for a wide range of tasks.\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "Deep learning is a subset of machine learning where neural networks—particularly those with many hidden layers—are used to model complex patterns in data. The \"deep\" in deep learning refers to the number of layers through which the data is transformed. More layers allow for more levels of abstraction and complexity in modeling the data.\n",
    "\n",
    "The concept of deep learning has been around for decades, but it wasn't until the mid-2000s that it gained significant traction. This was due to the availability of large amounts of data (big data) and substantial increases in computing power, particularly through the use of GPUs.\n",
    "\n",
    "Deep learning has led to significant advances in many areas, including computer vision, natural language processing, and reinforcement learning.\n",
    "\n",
    "Deep learning architectures include:\n",
    "\n",
    "- Deep neural networks (DNNs)\n",
    "- Convolutional neural networks (CNNs)\n",
    "- Recurrent neural networks (RNNs)\n",
    "- Long short-term memory networks (LSTMs)\n",
    "- Generative adversarial networks (GANs)\n",
    "\n",
    "## Historical Overview\n",
    "\n",
    "- **1950s-1960s**: Early research in neural networks, including the development of the perceptron.\n",
    "- **1970s**: The AI winter began, partly due to the limitations of neural networks at the time, such as the inability to solve non-linearly separable problems like the XOR problem.\n",
    "- **1980s**: The resurgence of neural networks with the introduction of backpropagation.\n",
    "- **1990s**: Support Vector Machines and other algorithms gained popularity over neural networks.\n",
    "- **Mid-2000s**: Deep learning started to become prominent due to advances in computing power and data availability.\n",
    "- **2010s-Present**: Deep learning has become a dominant approach in AI, achieving state-of-the-art results in various fields.\n",
    "\n",
    "The breakthrough came in the late 2000s and early 2010s when researchers started to train deep neural networks with much success, thanks to:\n",
    "\n",
    "- The availability of large datasets\n",
    "- Advances in computing power, particularly GPUs\n",
    "- Improvements in training algorithms and network architectures\n",
    "\n",
    "Since then, deep learning has been at the forefront of many AI breakthroughs and continues to push the boundaries of what machines can learn and accomplish.\n",
    "\n",
    "## Applications of Deep Learning\n",
    "\n",
    "Deep learning has a broad range of applications, many of which are revolutionizing industries and scientific research:\n",
    "\n",
    "- **Computer Vision**: Image and video recognition, object detection, image generation, and facial recognition\n",
    "- **Natural Language Processing (NLP)**: Language translation, sentiment analysis, and chatbots (including LLMs and ChatGPT)\n",
    "- **Speech Recognition**: Voice control systems, transcription, and real-time translation\n",
    "- **Healthcare**: Disease detection, drug discovery, and personalized medicine\n",
    "- **Autonomous Vehicles**: Self-driving cars and drones\n",
    "- **Gaming**: Non-player character (NPC) behavior, procedural content generation, and game testing\n",
    "- **Finance**: Fraud detection, algorithmic trading, and credit scoring\n",
    "- **Robotics**: Humanoid robots, industrial automation, and service robots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1122e7-0213-4d30-9f2c-376c01c908dc",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "The perceptron is one of the earliest and simplest models of a neural network. Developed in 1957 by Frank Rosenblatt, the perceptron was designed to mimic the way a single neuron in the brain processes information. It takes a set of inputs, applies a linear combination with weights and a bias, and outputs a binary result through an activation function.\n",
    "\n",
    "![neuron](../assets/neuron.png)\n",
    "\n",
    "It is a binary classifier that maps its input $ \\mathbf{x} $ (a real-valued vector) to an output value $ f(\\mathbf{x}) $ (a single binary value).\n",
    "\n",
    "The perceptron can only solve linearly separable problems, which is a significant limitation. However, its development laid the groundwork for more complex neural networks.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "![perceptron](../assets/perceptron.png)\n",
    "\n",
    "Given an input vector $ \\mathbf{x} $ and weight vector $ \\mathbf{w} $, the perceptron's output $ f(\\mathbf{x}) $ is defined as:\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\begin{cases} \n",
    "1 & \\text{if } \\mathbf{w} \\cdot \\mathbf{x} + b > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "where $ \\mathbf{w} \\cdot \\mathbf{x} $ is the dot product of the weights and input vector, and $ b $ is the bias term.\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "The perceptron learns by updating the weights and bias based on the errors made in predictions. The update rule is:\n",
    "\n",
    "$$ \\mathbf{w}_{(t+1)} = \\mathbf{w}_{(t)} + \\eta (y - \\hat{y}) \\mathbf{x} $$\n",
    "$$ b_{(t+1)} = b_{(t)} + \\eta (y - \\hat{y}) $$\n",
    "\n",
    "where $ \\eta $ is the learning rate, $ y $ is the true label, and $ \\hat{y} $ is the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc99f873-c097-4c3b-95b5-121676ed5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4cdc6199-b18a-42e4-af29-8cb3fd1b1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, no_of_inputs, epochs=100, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return 1 if summation > 0 else 0\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6910693d-e403-4d89-a928-8899d6a08da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])\n",
    "labels = np.array([1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8f487ce-cd2e-4fa4-a6c3-f6437d64f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(no_of_inputs=2)\n",
    "perceptron.train(training_inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc4096ca-79d9-4b30-9567-7cb00262a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(perceptron.predict(np.array([1, 1])))\n",
    "print(perceptron.predict(np.array([1, 0])))\n",
    "print(perceptron.predict(np.array([0, 1])))\n",
    "print(perceptron.predict(np.array([0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c9872-a4b9-4e34-8491-773ae4ddaa2d",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a class of feedforward artificial neural network that consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer. Unlike the Perceptron, which can only separate linearly separable classes, MLPs can classify non-linearly separable classes.\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a specific type of neural network and can be considered as one of the foundational architectures from which more complex neural network models have evolved.\n",
    "\n",
    "![mlp](../assets/multi_layer_perceptron.png)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Each neuron in the MLP applies an activation function $ \\phi $ to the weighted sum of its inputs. The output of the $ j $-th neuron in the $ l $-th layer is:\n",
    "\n",
    "$$ o_j^l = \\phi \\left( \\sum_{i} w_{ij}^l o_i^{l-1} + b_j^l \\right) $$\n",
    "\n",
    "where \n",
    "- $ w_{ij}^l $ is the weight connecting the $ i $-th neuron in the $ (l-1) $-th layer to the $ j $-th neuron in the $ l $-th layer\n",
    "- $ b_j^l $ is the bias term for the $ j $-th neuron in the $ l $-th layer\n",
    "- $ o_i^{l-1} $ is the output of the $ i $-th neuron in the $ (l-1) $-th layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947e966-96b7-4976-829d-bd74daf799eb",
   "metadata": {},
   "source": [
    "An MLP with at least one hidden layer and a sufficient number of neurons is a universal function approximator, as proven by the universal approximation theorem. This means that it can, in theory, learn any continuous function to an arbitrary level of accuracy, given enough neurons and data.\n",
    "\n",
    "### Non-linear Activation Functions\n",
    "\n",
    "Non-linear activation functions are crucial in MLPs (and neural networks), as they allow the network to capture complex patterns and interactions in the data. Without non-linearity, a neural network, regardless of how many layers it had, would still behave like a single-layer perceptron, only capable of modeling linear separations.\n",
    "\n",
    "A few common non-linear activation functions include:\n",
    "\n",
    "![sigmoid_tanh](../assets/sigmoid_tanh.jpeg)\n",
    "\n",
    "1. **Sigmoid Function**:\n",
    "   $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "   The sigmoid function squashes the input values to be between 0 and 1, which can be interpreted as probabilities. It is often used for the output layer in binary classification problems.\n",
    "\n",
    "2. **Hyperbolic Tangent Function (tanh)**:\n",
    "   $$ \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n",
    "   The tanh function outputs values between -1 and 1. It is zero-centered, which generally helps with the convergence during training.\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU)**:\n",
    "   $$ \\text{ReLU}(x) = \\max(0, x) $$\n",
    "   The ReLU function outputs the input directly if it is positive, otherwise, it outputs zero. It has become the default activation function for many types of neural networks because it helps with faster training and reduces the likelihood of vanishing gradients.\n",
    "\n",
    "![relu](../assets/relu.png)\n",
    "\n",
    "### Backpropagation Training\n",
    "\n",
    "Backpropagation is the algorithm used for training neural networks, including MLPs. It consists of two main phases: the forward pass and the backward pass.\n",
    "\n",
    "1. **Forward Pass (Feed-Forward)**:\n",
    "   During the forward pass, the input data is passed through the network layer by layer until the output layer is reached. The output is then used to compute the loss function, which measures the difference between the network's prediction and the actual target values. A common choice for the loss function in classification tasks is the cross-entropy loss.\n",
    "\n",
    "![ff](../assets/feedforward.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Backward Pass (Backpropagation)**:\n",
    "\n",
    "![bp](../assets/backprop.png)\n",
    "\n",
    "   In the backward pass, gradients of the loss function with respect to the network's weights are computed using the chain rule of calculus, in a process known as *gradient descent*.\n",
    "\n",
    "Backpropagation allows neural networks to learn from their mistakes, adjusting the weights to improve performance on the training data. With the right adjustments over many iterations, the network can converge to a state where it makes accurate predictions on unseen data.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent, as defined by the negative of the gradient. In the context of neural networks, gradient descent is used during backpropagation to update the weights and biases in order to minimize the loss function.\n",
    "\n",
    "![loss_functions](../assets/loss_functions.png)\n",
    "\n",
    "Let's consider a neural network with a loss function $ \\mathcal{L} $. The goal of training is to find the set of weights $ \\mathbf{W} $ and biases $ \\mathbf{b} $ that minimize $ \\mathcal{L} $. The gradient descent update rule for each weight $ W_{ij}^{(l)} $ in layer $ l $ is given by:\n",
    "\n",
    "$$ W_{ij}^{(l)} := W_{ij}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W_{ij}^{(l)}} $$\n",
    "\n",
    "Similarly, the update rule for each bias $ b_i^{(l)} $ is:\n",
    "\n",
    "$$ b_i^{(l)} := b_i^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b_i^{(l)}} $$\n",
    "\n",
    "![learning_rate](../assets/learning_rate.png)\n",
    "\n",
    "In these equations, $ \\eta $ is the learning rate, a hyperparameter that controls the size of the steps taken towards the minimum of $ \\mathcal{L} $. The partial derivatives $ \\frac{\\partial \\mathcal{L}}{\\partial W_{ij}^{(l)}} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b_i^{(l)}} $ represent the gradients of the loss function with respect to the weights and biases, respectively.\n",
    "\n",
    "During backpropagation, these gradients are computed using the **chain rule**. For a given layer $ l $, the gradient of the loss with respect to the activation $ o_i^{(l)} $ is calculated based on the gradients from the subsequent layer $ l+1 $:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial o_i^{(l)}} = \\sum_{k} \\frac{\\partial \\mathcal{L}}{\\partial z_k^{(l+1)}} \\frac{\\partial z_k^{(l+1)}}{\\partial o_i^{(l)}} $$\n",
    "\n",
    "where $ z_k^{(l+1)} $ is the weighted input to the activation function in the next layer, and the sum is over all neurons $ k $ in layer $ l+1 $ that receive inputs from neuron $ i $ in layer $ l $.\n",
    "\n",
    "The gradient of the loss with respect to the weighted input $ z_k^{(l+1)} $ is then calculated by applying the derivative of the activation function $ \\phi $ used in layer $ l+1 $:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial z_k^{(l+1)}} = \\frac{\\partial \\mathcal{L}}{\\partial o_k^{(l+1)}} \\phi'(z_k^{(l+1)}) $$\n",
    "\n",
    "Finally, the gradients with respect to the weights and biases are computed as:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{ij}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial z_i^{(l)}} o_j^{(l-1)} $$\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial b_i^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial z_i^{(l)}} $$\n",
    "\n",
    "These gradients are used to update the weights and biases in the opposite direction of the gradient, hence the term \"gradient descent.\"\n",
    "\n",
    "![gradient_descent](../assets/gradient_descent.png)\n",
    "\n",
    "Adjusting the weights consists of multiple iterations. A new step is taken for each iteration and calculate a new weight. Using the initial weight and the gradient and learning rate, the subsequent weights can be determined.\n",
    "\n",
    "There are several variants of gradient descent, including:\n",
    "\n",
    "- **Batch Gradient Descent**: Computes the gradient of the loss function with respect to the parameters for the entire training dataset.\n",
    "- **Stochastic Gradient Descent (SGD)**: Computes the gradient for each training example and updates the parameters accordingly.\n",
    "- **Mini-batch Gradient Descent**: A compromise between batch and stochastic versions, which computes the gradient against a subset of the training data at each step.\n",
    "\n",
    "Choosing the right variant and learning rate requires careful tuning and is critical for the successful training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53f138-ad36-49b1-bcfc-31896714d9b0",
   "metadata": {},
   "source": [
    "### Tutorial\n",
    "\n",
    "Let's implement a simple neural network that utilizes feedforward and backpropagation algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d84adf-13de-44a7-9df9-d730b99bb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Define network architecture\n",
    "input_size = 2  # Number of features\n",
    "hidden_size = 2  # Number of neurons in the hidden layer\n",
    "output_size = 1  # Number of neurons in the output layer\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.rand(input_size, hidden_size)\n",
    "b1 = np.random.rand(hidden_size)\n",
    "W2 = np.random.rand(hidden_size, output_size)\n",
    "b2 = np.random.rand(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2762f07b-84ba-443f-b0b6-db8b887892d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the sigmoid function as our activation function.\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2426eeae-e11a-4338-9895-3c4b0d71ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(X, W1, b1, W2, b):\n",
    "    # Input to hidden layer\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)  # Activation in hidden layer\n",
    "    \n",
    "    # Hidden layer to output\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)  # Activation in output layer\n",
    "    \n",
    "    return A1, A2, Z1, Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7ec173-c4ab-4e4e-a96f-d7198ee8cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the mean squared error (MSE) loss function.\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d25d3f-fc24-427f-87ee-5b1964611ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, y_true, A1, A2, Z1, Z2, W1, b1, W2, b2, learning_rate=0.1):\n",
    "    m = y_true.shape[0]  # Number of examples\n",
    "    \n",
    "    # Calculate derivatives\n",
    "    dZ2 = A2 - y_true  # Derivative of loss with respect to Z2\n",
    "    dW2 = (1 / m) * np.dot(A1.T, dZ2)  # Derivative of loss with respect to W2\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=0)  # Derivative of loss with respect to b2\n",
    "    \n",
    "    dA1 = np.dot(dZ2, W2.T)  # Derivative of loss with respect to A1\n",
    "    dZ1 = dA1 * sigmoid_derivative(Z1)  # Derivative of loss with respect to Z1 (sigmoid prime)\n",
    "    dW1 = (1 / m) * np.dot(X.T, dZ1)  # Derivative of loss with respect to W1\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=0)  # Derivative of loss with respect to b1\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635d8654-c16c-45bb-93d8-3e6afb04fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the feedforward and backpropagation functions to train the network on some dummy data.\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_true = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "310497d5-a1a8-4f8a-9314-1e96def701dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.3247\n",
      "Epoch 1000: Loss = 0.2406\n",
      "Epoch 2000: Loss = 0.1940\n",
      "Epoch 3000: Loss = 0.1169\n",
      "Epoch 4000: Loss = 0.0180\n",
      "Epoch 5000: Loss = 0.0041\n",
      "Epoch 6000: Loss = 0.0016\n",
      "Epoch 7000: Loss = 0.0009\n",
      "Epoch 8000: Loss = 0.0005\n",
      "Epoch 9000: Loss = 0.0003\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward\n",
    "    A1, A2, Z1, Z2 = feedforward(X, W1, b1, W2, b2)\n",
    "    \n",
    "    # Backpropagation\n",
    "    W1, b1, W2, b2 = backprop(X, y_true, A1, A2, Z1, Z2, W1, b1, W2, b2)\n",
    "    \n",
    "    # Calculate loss (mean squared error)\n",
    "    loss = np.mean((y_true - A2) ** 2)\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}: Loss = {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f2d423-f7df-4863-91c2-83b4effb221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output:\n",
      "[[0.01838   ]\n",
      " [0.98577795]\n",
      " [0.98574084]\n",
      " [0.01564593]]\n"
     ]
    }
   ],
   "source": [
    "A1, A2, Z1, Z2 = feedforward(X, W1, b1, W2, b2)\n",
    "print('Predicted output:')\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295634a-4426-46a2-80bb-48449efc5bed",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR). It is one of the most popular deep learning frameworks. It is widely used for applications such as computer vision and natural language processing and is known for its flexibility, ease of use, and dynamic computational graph. PyTorch is particularly popular in the research community due to its flexibility and ease of experimentation, but it is also gaining traction in the industry for development and deployment of machine learning models.\n",
    "\n",
    "## Key Features of PyTorch:\n",
    "\n",
    "- **Dynamic Computation Graph**: Also known as define-by-run paradigm, where the graph is built on the fly as operations are created. This feature makes it easy to change the network architecture during runtime and is beneficial for working with variable-length inputs and outputs.\n",
    "\n",
    "- **Pythonic Nature**: PyTorch is deeply integrated with Python, making it intuitive to learn and use. It leverages the power of Python's libraries and allows for easy debugging using standard Python debugging tools.\n",
    "\n",
    "- **GPU Acceleration**: PyTorch supports CUDA, which allows it to efficiently compute forward and backward passes on GPUs for faster training of models.\n",
    "\n",
    "- **TorchScript**: A way to create serializable and optimizable models from PyTorch code. This allows for models to be run independently from Python, which is useful for deploying models in production environments.\n",
    "\n",
    "- **Extensive Library**: PyTorch includes a wide range of tools and libraries for various tasks in machine learning, like torchvision for computer vision, torchaudio for audio processing, and torchtext for natural language processing.\n",
    "\n",
    "- **Community and Ecosystem**: PyTorch has a large and active community that contributes to a growing ecosystem of tools, libraries, and extensions.\n",
    "\n",
    "- **Autograd Module**: Provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that the backpropagation is defined by how your code is run, and every single iteration can be different.\n",
    "\n",
    "- **Distributed Training**: PyTorch supports native distributed training, making it easy to scale computation across multiple GPUs and machines.\n",
    "\n",
    "- **Pretrained Models**: PyTorch provides a number of pre-trained models which can be used for transfer learning, fine-tuning, or as a starting point for custom solutions.\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "### Negative Log Likelihood (NLL) Loss\n",
    "\n",
    "Negative Log Likelihood Loss is a loss function commonly used in classification tasks, particularly when the output of the neural network represents the log probabilities of the classes. It is often used in conjunction with the softmax layer that normalizes the network's raw output scores into probabilities.\n",
    "\n",
    "The NLL loss is defined as:\n",
    "\n",
    "$$ L(y, \\hat{y}) = -\\sum_{i=1}^{N} \\log(\\hat{y}_i[y_i]) $$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of samples,\n",
    "- $y$ is the true label (index of the correct class),\n",
    "- $\\hat{y}_i$ is the predicted probability of the $i$-th sample for its true class $y_i$.\n",
    "\n",
    "In PyTorch, the NLL loss is implemented as `torch.nn.NLLLoss`. It expects the inputs to be log probabilities, which can be obtained by applying the `torch.log` function to the output of a softmax layer or, more commonly, by using the `torch.nn.LogSoftmax` layer.\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "Cross-Entropy Loss combines LogSoftmax and NLL Loss in one single class. It is useful when training a classification problem with C classes. It is defined as:\n",
    "\n",
    "$$ L(y, \\hat{y}) = -\\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{i,j} \\log(\\hat{y}_{i,j}) $$\n",
    "\n",
    "where $y_{i,j}$ is a binary indicator of whether class label $j$ is the correct classification for observation $i$, and $\\hat{y}_{i,j}$ is the predicted probability of observation $i$ being of class $j$.\n",
    "\n",
    "In PyTorch, it is implemented as `torch.nn.CrossEntropyLoss`.\n",
    "\n",
    "### Mean Squared Error Loss\n",
    "\n",
    "Mean Squared Error (MSE) Loss is used for regression tasks and measures the squared difference between the target and the output:\n",
    "\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "In PyTorch, it is implemented as `torch.nn.MSELoss`.\n",
    "\n",
    "### Binary Cross-Entropy Loss\n",
    "\n",
    "Binary Cross-Entropy Loss is used for binary classification tasks:\n",
    "\n",
    "$$ L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right] $$\n",
    "\n",
    "In PyTorch, it is implemented as `torch.nn.BCELoss`. For outputs that are probabilities, you would use `torch.nn.BCEWithLogitsLoss`, which combines a sigmoid layer and the BCELoss in one single class.\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "Optimizers are algorithms or methods used to change the attributes of a neural network, such as weights and learning rate, to reduce the losses. Optimizers help to minimize the loss function. Optimizers guide the training process by telling the network how to update its weights via backpropagation. Gradient Descent that we learnt above is one of the optimizers. The goal of optimization is to find the best parameters for the model to perform the task at hand.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "SGD is the most classical approach to fitting linear classifiers and convex functions. It's defined by the update rule:\n",
    "\n",
    "$$ \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta; x^{(i)}; y^{(i)}) $$\n",
    "\n",
    "where $\\theta$ is the parameter, $\\eta$ is the learning rate, and $\\nabla_{\\theta}J$ is the gradient of the loss function $J$ with respect to the parameter $\\theta$.\n",
    "\n",
    "In PyTorch, SGD is implemented as `torch.optim.SGD`.\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "RMSprop is an adaptive learning rate method. It divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n",
    "\n",
    "In PyTorch, RMSprop is implemented as `torch.optim.RMSprop`.\n",
    "\n",
    "### Adagrad\n",
    "\n",
    "Adagrad adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. It's well-suited for dealing with sparse data.\n",
    "\n",
    "In PyTorch, Adagrad is implemented as `torch.optim.Adagrad`.\n",
    "\n",
    "### Adam Optimizer\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimizer that can be used instead of the classical stochastic gradient descent (SGD) to update network weights iteratively based on training data. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "\n",
    "Key Features of Adam:\n",
    "\n",
    "- Adaptive Learning Rate: Maintains a learning rate for each network weight (parameter) and separately adapts them as learning unfolds.\n",
    "- Computationally Efficient: Requires little memory and is invariant to diagonal rescale of the gradients.\n",
    "- Well Suited for Problems: That are large in terms of data and/or parameters.\n",
    "- Appropriate for Non-Stationary Objectives: And problems with very noisy and/or sparse gradients.\n",
    "\n",
    "In PyTorch, the Adam optimizer is implemented as `torch.optim.Adam`.\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. It does this by restricting the window of accumulated past gradients to some fixed size.\n",
    "\n",
    "In PyTorch, Adadelta is implemented as `torch.optim.Adadelta`.\n",
    "\n",
    "### Adamax\n",
    "\n",
    "Adamax is a variant of Adam based on the infinity norm. It can be seen as a generalization of Adam that is more stable in cases where the gradients' infinity norm is very large or very small.\n",
    "\n",
    "In PyTorch, Adamax is implemented as `torch.optim.Adamax`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc4021-8f24-416e-ac6d-7662de55ab49",
   "metadata": {},
   "source": [
    "## Building a MLP (Fully Connected Neural Network) with PyTorch\n",
    "\n",
    "We will learn how to build, train, and evaluate a MLP (also known as fully connected neural network) using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3c20d3-0449-4419-aaa7-0bdb4f29acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e482e",
   "metadata": {},
   "source": [
    "Let's use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), which consists of 28x28 pixel grayscale images of handwritten digits (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1078fd3-7fa9-45e8-a072-27f6540f67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f090e-163a-4220-8420-c5d367463bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple neural network with two hidden layers.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Flatten the image\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # Output layer, 10 units - one for each digit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # Flatten the input tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "511b2aa7-fb89-4622-a986-aeaccc6f8895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10d97263-0385-44d8-a0d1-484d8567eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the negative log-likelihood loss and the Adam optimizer.\n",
    "\n",
    "# Negative log-likelihood loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6092edd-c266-484a-a1b4-a9c35bbf82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.32406119955405754\n",
      "Training loss: 0.1631146809016702\n",
      "Training loss: 0.13455375534162592\n",
      "Training loss: 0.11795742112074865\n",
      "Training loss: 0.10921651368582451\n"
     ]
    }
   ],
   "source": [
    "# Train the network for a few epochs.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss / len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fca079d2-e8c8-4ccc-9162-010583148196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.08%\n"
     ]
    }
   ],
   "source": [
    "# Let's check the performance of our network on the test dataset.\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Turn off gradients for validation to save memory and computations\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        output = model(images)\n",
    "        # print(output)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {correct / total * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95fc47-ea4b-4950-b37f-64d801910913",
   "metadata": {},
   "source": [
    "# Building a Fully Connected Neural Network on the Titanic Dataset\n",
    "\n",
    "In this tutorial, we will build a fully connected neural network using PyTorch to predict the survival of passengers from the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ebb9f6e-1648-49bd-9318-2a844bc2a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e554e90b-f21c-4b23-b8fc-31b3a81a909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "091dbc31-2cc5-406d-b97e-2ba453499d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "0      man        True  NaN  Southampton    no  False  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "2    woman       False  NaN  Southampton   yes   True  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "4      man        True  NaN  Southampton    no   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "886    man        True  NaN  Southampton    no   True  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "888  woman       False  NaN  Southampton    no  False  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "890    man        True  NaN   Queenstown    no   True  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77c68f68-367a-476f-a80d-cf957c00a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['survived', 'pclass', 'sex', 'age', 'fare', 'embarked']\n",
    "data = titanic[columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3581b33c-a17f-4e6f-b5af-e4092515df76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88b6f49d-7fa3-4140-8554-7c9442d0aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['embarked'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e099c21-5ad4-4921-b8c3-b1c52cc25c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16628f80-31a2-4faa-acb3-ddb3096ec964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "num_features = ['age', 'fare']\n",
    "cat_features = ['pclass', 'sex', 'embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ca9685c-bb1b-42f5-beb5-2b559dc87a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessing pipelines for both numerical and categorical data\n",
    "num_pipeline = SimpleImputer(strategy='mean')\n",
    "cat_pipeline = OneHotEncoder()\n",
    "\n",
    "# Combine pipelines into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_processed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bfc7512-41f8-4b6b-95e4-384836750ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler(with_mean=False)  # We use with_mean=False to work with sparse matrix\n",
    "X_scaled = scaler.fit_transform(X_processed)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17dfefc2-10b0-4795-b6a5-4d76e0d74eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # Convert sparse matrix to dense\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fce69cf-80df-4994-8b34-29be0f385ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dc4f55b-4d64-4005-aad7-95170f69aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6111673b-e022-43f7-8d12-11176060994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TitanicNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2699a60-bb2f-4f77-bb86-b47ff89f070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 30  # You can tune this parameter\n",
    "output_size = 1\n",
    "\n",
    "model = TitanicNet(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2088a5e2-0287-46d5-be10-f2df54562a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f077330-8c64-4973-94d4-523995213ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000], Loss: 0.5554\n",
      "Epoch [40/1000], Loss: 0.4944\n",
      "Epoch [60/1000], Loss: 0.4944\n",
      "Epoch [80/1000], Loss: 0.2369\n",
      "Epoch [100/1000], Loss: 0.6332\n",
      "Epoch [120/1000], Loss: 0.2255\n",
      "Epoch [140/1000], Loss: 0.1302\n",
      "Epoch [160/1000], Loss: 0.3154\n",
      "Epoch [180/1000], Loss: 0.0743\n",
      "Epoch [200/1000], Loss: 0.0834\n",
      "Epoch [220/1000], Loss: 0.3998\n",
      "Epoch [240/1000], Loss: 0.2371\n",
      "Epoch [260/1000], Loss: 0.2954\n",
      "Epoch [280/1000], Loss: 0.3562\n",
      "Epoch [300/1000], Loss: 0.6157\n",
      "Epoch [320/1000], Loss: 0.5892\n",
      "Epoch [340/1000], Loss: 0.2519\n",
      "Epoch [360/1000], Loss: 0.2571\n",
      "Epoch [380/1000], Loss: 0.3259\n",
      "Epoch [400/1000], Loss: 0.2396\n",
      "Epoch [420/1000], Loss: 0.1099\n",
      "Epoch [440/1000], Loss: 0.1426\n",
      "Epoch [460/1000], Loss: 0.2590\n",
      "Epoch [480/1000], Loss: 0.3401\n",
      "Epoch [500/1000], Loss: 0.2600\n",
      "Epoch [520/1000], Loss: 0.2403\n",
      "Epoch [540/1000], Loss: 1.1586\n",
      "Epoch [560/1000], Loss: 0.4127\n",
      "Epoch [580/1000], Loss: 0.3202\n",
      "Epoch [600/1000], Loss: 0.1049\n",
      "Epoch [620/1000], Loss: 0.2905\n",
      "Epoch [640/1000], Loss: 0.1955\n",
      "Epoch [660/1000], Loss: 0.2870\n",
      "Epoch [680/1000], Loss: 0.2473\n",
      "Epoch [700/1000], Loss: 0.1435\n",
      "Epoch [720/1000], Loss: 0.0928\n",
      "Epoch [740/1000], Loss: 1.0262\n",
      "Epoch [760/1000], Loss: 0.0520\n",
      "Epoch [780/1000], Loss: 0.6004\n",
      "Epoch [800/1000], Loss: 0.5652\n",
      "Epoch [820/1000], Loss: 0.0965\n",
      "Epoch [840/1000], Loss: 0.0979\n",
      "Epoch [860/1000], Loss: 0.2350\n",
      "Epoch [880/1000], Loss: 0.2866\n",
      "Epoch [900/1000], Loss: 0.5310\n",
      "Epoch [920/1000], Loss: 0.1755\n",
      "Epoch [940/1000], Loss: 0.5566\n",
      "Epoch [960/1000], Loss: 0.3135\n",
      "Epoch [980/1000], Loss: 0.2155\n",
      "Epoch [1000/1000], Loss: 0.2449\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        labels = labels.view(-1, 1)  # Reshape the labels to match the output\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8413a384-6cbd-45b5-a3b1-3603ef0e8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        label = (outputs.data >= 0.5).float()\n",
    "        y_preds.extend(label.flatten().numpy().tolist())\n",
    "\n",
    "y_preds = np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "129e6533-2d5b-47f0-9690-833e84af11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_preds)\n",
    "precision = precision_score(y_test, y_preds)\n",
    "recall = recall_score(y_test, y_preds)\n",
    "f1 = f1_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca9bdf31-6189-4d3c-8dc6-b2bc61946f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Precision: 0.75\n",
      "Recall: 0.55\n",
      "F1 Score: 0.63\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
